{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask_ml.preprocessing import Categorizer\n",
    "from glum import GeneralizedLinearRegressor, TweedieDistribution\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, SplineTransformer, StandardScaler\n",
    "#import everything here in the masterfile\n",
    "#kernels sometimes need to be restarted when new lib installed\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Getting project root\n",
    "project_root = Path.cwd().resolve().parent.parent\n",
    "\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "from ps3.data import create_sample_split, load_transform\n",
    "#from ps3.data import train_tweedie, model_pipeline, spline_model\n",
    "#only if code reworked modularly, currently not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = load_transform()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to ensure IDpol column is unique - we shall carry out split on this\n",
    "len(df[\"IDpol\"].unique()) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train benchmark tweedie model. This is entirely based on the glum tutorial.\n",
    "#weight = df[\"Exposure\"].values\n",
    "df[\"PurePremium\"] = df[\"ClaimAmountCut\"] / df[\"Exposure\"]\n",
    "\n",
    "outcome = \"PurePremium\"\n",
    "# TODO: Why do you think, we divide by exposure here to arrive at our outcome variable?\n",
    "'''\n",
    "To get a yearly figure so we can compare claims that have a different time under risk\n",
    "'''\n",
    "\n",
    "\n",
    "# TODO: use your create_sample_split function here\n",
    "df_split = create_sample_split(df,\"IDpol\",0.8)\n",
    "df_train = df_split[0].copy()\n",
    "df_test = df_split[1].copy()\n",
    "\n",
    "\n",
    "categoricals = [\"VehBrand\", \"VehGas\", \"Region\", \"Area\", \"DrivAge\", \"VehAge\", \"VehPower\"]\n",
    "\n",
    "predictors = categoricals + [\"BonusMalus\", \"Density\"]\n",
    "glm_categorizer = Categorizer(columns=categoricals)\n",
    "\n",
    "X_train_t = glm_categorizer.fit_transform(df_train[predictors])\n",
    "X_test_t = glm_categorizer.transform(df_test[predictors])\n",
    "y_train_t, y_test_t = df_train[outcome], df_test[outcome]\n",
    "w_train_t, w_test_t = df_train[\"Exposure\"], df_test[\"Exposure\"]\n",
    "\n",
    "TweedieDist = TweedieDistribution(1.5)\n",
    "t_glm1 = GeneralizedLinearRegressor(family=TweedieDist, l1_ratio=1, fit_intercept=True)\n",
    "t_glm1.fit(X_train_t, y_train_t, sample_weight=w_train_t)\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"coefficient\": np.concatenate(([t_glm1.intercept_], t_glm1.coef_))},\n",
    "    index=[\"intercept\"] + t_glm1.feature_names_,\n",
    ").T\n",
    "\n",
    "df_test.loc[:,\"pp_t_glm1\"] = t_glm1.predict(X_test_t)\n",
    "df_train.loc[:,\"pp_t_glm1\"] = t_glm1.predict(X_train_t)\n",
    "\n",
    "print(\n",
    "    \"training loss t_glm1:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_glm1\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_glm1:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_glm1\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Total claim amount on test set, observed = {}, predicted = {}\".format(\n",
    "        df_test[\"ClaimAmountCut\"].values.sum(),\n",
    "        np.sum(df_test[\"Exposure\"].values * t_glm1.predict(X_test_t)),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's add splines for BonusMalus and Density and use a Pipeline.\n",
    "# Steps: \n",
    "# 1. Define a Pipeline which chains a StandardScaler and SplineTransformer. \n",
    "#    Choose knots=\"quantile\" for the SplineTransformer and make sure, we \n",
    "#    are only including one intercept in the final GLM. \n",
    "# 2. Put the transforms together into a ColumnTransformer. Here we use OneHotEncoder for the categoricals.\n",
    "# 3. Chain the transforms together with the GLM in a Pipeline.\n",
    "categoricals = [\"VehBrand\", \"VehGas\", \"Region\", \"Area\", \"DrivAge\", \"VehAge\", \"VehPower\"]\n",
    "# Let's put together a pipeline\n",
    "numeric_cols = [\"BonusMalus\", \"Density\"]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # TODO: Add numeric transforms here\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, drop=\"first\"), categoricals),\n",
    "        (\"spline\", SplineTransformer(knots=\"quantile\"), numeric_cols),\n",
    "        (\"scale\", StandardScaler(), numeric_cols)\n",
    "    ]\n",
    ")\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "model_pipeline = Pipeline(\n",
    "    # TODO: Define pipeline steps here\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\n",
    "            \"estimate\",\n",
    "            GeneralizedLinearRegressor(\n",
    "                family=TweedieDist, l1_ratio=1, fit_intercept=True\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#incorporate this part later\n",
    "# let's have a look at the pipeline\n",
    "model_pipeline\n",
    "\n",
    "# let's check that the transforms worked\n",
    "model_pipeline[:-1].fit_transform(df_train)\n",
    "\n",
    "model_pipeline.fit(df_train, y_train_t, estimate__sample_weight=w_train_t)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"coefficient\": np.concatenate(\n",
    "            ([model_pipeline[-1].intercept_], model_pipeline[-1].coef_)\n",
    "        )\n",
    "    },\n",
    "    index=[\"intercept\"] + model_pipeline[-1].feature_names_,\n",
    ").T\n",
    "\n",
    "df_test.loc[:,\"pp_t_glm2\"] = model_pipeline.predict(df_test)\n",
    "df_train.loc[:,\"pp_t_glm2\"] = model_pipeline.predict(df_train)\n",
    "\n",
    "print(\n",
    "    \"training loss t_glm2:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_glm2\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_glm2:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_glm2\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n",
    "# Create Boolean mask for test rows \n",
    "test = df[\"sample\"] == \"test\"\n",
    "print(\n",
    "    \"Total claim amount on test set, observed = {}, predicted = {}\".format(\n",
    "        df[\"ClaimAmountCut\"].values[test].sum(),\n",
    "        np.sum(df[\"Exposure\"].values[test] * df_test[\"pp_t_glm2\"]),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's use a GBM instead as an estimator.\n",
    "# Steps\n",
    "# 1: Define the modelling pipeline. Tip: This can simply be a LGBMRegressor based on X_train_t from before.\n",
    "# 2. Make sure we are choosing the correct objective for our estimator.\n",
    "model_pipeline = Pipeline([(\"estimate\", LGBMRegressor(objective=\"tweedie\"))])\n",
    "model_pipeline.fit(X_train_t, y_train_t, estimate__sample_weight=w_train_t)\n",
    "df_test.loc[:,\"pp_t_lgbm\"] = model_pipeline.predict(X_test_t)\n",
    "df_train.loc[:,\"pp_t_lgbm\"] = model_pipeline.predict(X_train_t)\n",
    "print(\n",
    "    \"training loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_lgbm\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_lgbm\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n",
    "#substantial difference between losses, might overfit for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's tune the LGBM to reduce overfitting.\n",
    "# Steps:\n",
    "# 1. Define a `GridSearchCV` object with our lgbm pipeline/estimator. Tip: Parameters for a specific step of the pipeline\n",
    "# can be passed by <step_name>__param. \n",
    "\n",
    "# Note: Typically we tune many more parameters and larger grids,\n",
    "# but to save compute time here, we focus on getting the learning rate\n",
    "# and the number of estimators somewhat aligned -> tune learning_rate and n_estimators\n",
    "cv = GridSearchCV(\n",
    "    model_pipeline,\n",
    "    {\n",
    "        \"estimate__learning_rate\": [0.01, 0.02, 0.03, 0.04, 0.05, 0.1],\n",
    "        \"estimate__n_estimators\": [50, 100, 150, 200],\n",
    "    },\n",
    "    verbose=2,\n",
    ")\n",
    "cv.fit(X_train_t, y_train_t, estimate__sample_weight=w_train_t)\n",
    "\n",
    "df_test.loc[:,\"pp_t_lgbm\"] = cv.best_estimator_.predict(X_test_t)\n",
    "df_train.loc[:,\"pp_t_lgbm\"] = cv.best_estimator_.predict(X_train_t)\n",
    "\n",
    "print(\n",
    "    \"training loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_lgbm\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_lgbm\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Total claim amount on test set, observed = {}, predicted = {}\".format(\n",
    "        df[\"ClaimAmountCut\"].values[test].sum(),\n",
    "        np.sum(df[\"Exposure\"].values[test] * df_test[\"pp_t_lgbm\"]),\n",
    "    )\n",
    ")\n",
    "#[CV] END estimate__learning_rate=0.1, estimate__n_estimators=200; total time=   2.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the sorting of the pure premium predictions\n",
    "# Source: https://scikit-learn.org/stable/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html\n",
    "def lorenz_curve(y_true, y_pred, exposure):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    exposure = np.asarray(exposure)\n",
    "\n",
    "    # order samples by increasing predicted risk:\n",
    "    ranking = np.argsort(y_pred)\n",
    "    ranked_exposure = exposure[ranking]\n",
    "    ranked_pure_premium = y_true[ranking]\n",
    "    cumulated_claim_amount = np.cumsum(ranked_pure_premium * ranked_exposure)\n",
    "    cumulated_claim_amount /= cumulated_claim_amount[-1]\n",
    "    cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))\n",
    "    return cumulated_samples, cumulated_claim_amount\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "for label, y_pred in [\n",
    "    (\"LGBM\", df_test[\"pp_t_lgbm\"]),\n",
    "    (\"GLM Benchmark\", df_test[\"pp_t_glm1\"]),\n",
    "    (\"GLM Splines\", df_test[\"pp_t_glm2\"]),\n",
    "]:\n",
    "    ordered_samples, cum_claims = lorenz_curve(\n",
    "        df_test[\"PurePremium\"], y_pred, df_test[\"Exposure\"]\n",
    "    )\n",
    "    gini = 1 - 2 * auc(ordered_samples, cum_claims)\n",
    "    label += f\" (Gini index: {gini: .3f})\"\n",
    "    ax.plot(ordered_samples, cum_claims, linestyle=\"-\", label=label)\n",
    "\n",
    "# Oracle model: y_pred == y_test\n",
    "ordered_samples, cum_claims = lorenz_curve(\n",
    "    df_test[\"PurePremium\"], df_test[\"PurePremium\"], df_test[\"Exposure\"]\n",
    ")\n",
    "gini = 1 - 2 * auc(ordered_samples, cum_claims)\n",
    "label = f\"Oracle (Gini index: {gini: .3f})\"\n",
    "ax.plot(ordered_samples, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)\n",
    "\n",
    "# Random baseline\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\n",
    "ax.set(\n",
    "    title=\"Lorenz Curves\",\n",
    "    xlabel=\"Fraction of policyholders\\n(ordered by model from safest to riskiest)\",\n",
    "    ylabel=\"Fraction of total claim amount\",\n",
    ")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
