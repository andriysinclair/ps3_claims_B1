{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask_ml.preprocessing import Categorizer\n",
    "from glum import GeneralizedLinearRegressor, TweedieDistribution\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, SplineTransformer, StandardScaler\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Getting project root\n",
    "project_root = Path.cwd().resolve().parent\n",
    "\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "from ps3.data import create_sample_split, load_transform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = load_transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDpol</th>\n",
       "      <th>ClaimNb</th>\n",
       "      <th>Exposure</th>\n",
       "      <th>Area</th>\n",
       "      <th>VehPower</th>\n",
       "      <th>VehAge</th>\n",
       "      <th>DrivAge</th>\n",
       "      <th>BonusMalus</th>\n",
       "      <th>VehBrand</th>\n",
       "      <th>VehGas</th>\n",
       "      <th>Density</th>\n",
       "      <th>Region</th>\n",
       "      <th>ClaimAmount</th>\n",
       "      <th>ClaimAmountCut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Regular</td>\n",
       "      <td>1217</td>\n",
       "      <td>R82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Regular</td>\n",
       "      <td>1217</td>\n",
       "      <td>R82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>B</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>54</td>\n",
       "      <td>R22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>B</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>76</td>\n",
       "      <td>R72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>B</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>76</td>\n",
       "      <td>R72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDpol  ClaimNb  Exposure Area  VehPower  VehAge  DrivAge  BonusMalus  \\\n",
       "0      1        0      0.10    D         5       0        5          50   \n",
       "1      3        0      0.77    D         5       0        5          50   \n",
       "2      5        0      0.75    B         6       1        5          50   \n",
       "3     10        0      0.09    B         7       0        4          50   \n",
       "4     11        0      0.84    B         7       0        4          50   \n",
       "\n",
       "  VehBrand   VehGas  Density Region  ClaimAmount  ClaimAmountCut  \n",
       "0      B12  Regular     1217    R82          0.0             0.0  \n",
       "1      B12  Regular     1217    R82          0.0             0.0  \n",
       "2      B12   Diesel       54    R22          0.0             0.0  \n",
       "3      B12   Diesel       76    R72          0.0             0.0  \n",
       "4      B12   Diesel       76    R72          0.0             0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing head\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to ensure IDpol column is unique - we shall carry out split on this\n",
    "len(df[\"IDpol\"].unique()) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678013"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35818/3258302099.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"pp_t_glm1\"] = t_glm1.predict(X_test_t)\n",
      "/tmp/ipykernel_35818/3258302099.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"pp_t_glm1\"] = t_glm1.predict(X_train_t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss t_glm1:  73.69714563112684\n",
      "testing loss t_glm1:  74.02897586575135\n",
      "Total claim amount on test set, observed = 9824331.800000003, predicted = 10031127.336040936\n"
     ]
    }
   ],
   "source": [
    "# Train benchmark tweedie model. This is entirely based on the glum tutorial.\n",
    "#weight = df[\"Exposure\"].values\n",
    "df[\"PurePremium\"] = df[\"ClaimAmountCut\"] / df[\"Exposure\"]\n",
    "\n",
    "outcome = \"PurePremium\"\n",
    "# TODO: Why do you think, we divide by exposure here to arrive at our outcome variable?\n",
    "'''\n",
    "To get a yearly figure so we can compare claims that have a different time under risk\n",
    "'''\n",
    "\n",
    "\n",
    "# TODO: use your create_sample_split function here\n",
    "df_split = create_sample_split(df,\"IDpol\",0.8)\n",
    "df_train = df_split[0]\n",
    "df_test = df_split[1]\n",
    "\n",
    "\n",
    "categoricals = [\"VehBrand\", \"VehGas\", \"Region\", \"Area\", \"DrivAge\", \"VehAge\", \"VehPower\"]\n",
    "\n",
    "predictors = categoricals + [\"BonusMalus\", \"Density\"]\n",
    "glm_categorizer = Categorizer(columns=categoricals)\n",
    "\n",
    "X_train_t = glm_categorizer.fit_transform(df_train[predictors])\n",
    "X_test_t = glm_categorizer.transform(df_test[predictors])\n",
    "y_train_t, y_test_t = df_train[outcome], df_test[outcome]\n",
    "w_train_t, w_test_t = df_train[\"Exposure\"], df_test[\"Exposure\"]\n",
    "\n",
    "TweedieDist = TweedieDistribution(1.5)\n",
    "t_glm1 = GeneralizedLinearRegressor(family=TweedieDist, l1_ratio=1, fit_intercept=True)\n",
    "t_glm1.fit(X_train_t, y_train_t, sample_weight=w_train_t)\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"coefficient\": np.concatenate(([t_glm1.intercept_], t_glm1.coef_))},\n",
    "    index=[\"intercept\"] + t_glm1.feature_names_,\n",
    ").T\n",
    "\n",
    "df_test[\"pp_t_glm1\"] = t_glm1.predict(X_test_t)\n",
    "df_train[\"pp_t_glm1\"] = t_glm1.predict(X_train_t)\n",
    "\n",
    "print(\n",
    "    \"training loss t_glm1:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_glm1\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_glm1:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_glm1\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Total claim amount on test set, observed = {}, predicted = {}\".format(\n",
    "        df_test[\"ClaimAmountCut\"].values.sum(),\n",
    "        np.sum(df_test[\"Exposure\"].values * t_glm1.predict(X_test_t)),\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's add splines for BonusMalus and Density and use a Pipeline.\n",
    "# Steps: \n",
    "# 1. Define a Pipeline which chains a StandardScaler and SplineTransformer. \n",
    "#    Choose knots=\"quantile\" for the SplineTransformer and make sure, we \n",
    "#    are only including one intercept in the final GLM. \n",
    "# 2. Put the transforms together into a ColumnTransformer. Here we use OneHotEncoder for the categoricals.\n",
    "# 3. Chain the transforms together with the GLM in a Pipeline.\n",
    "\n",
    "# Let's put together a pipeline\n",
    "numeric_cols = [\"BonusMalus\", \"Density\"]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # TODO: Add numeric transforms here\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, drop=\"first\"), categoricals),\n",
    "    ]\n",
    ")\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "model_pipeline = Pipeline(\n",
    "    # TODO: Define pipeline steps here\n",
    ")\n",
    "\n",
    "# let's have a look at the pipeline\n",
    "model_pipeline\n",
    "\n",
    "# let's check that the transforms worked\n",
    "model_pipeline[:-1].fit_transform(df_train)\n",
    "\n",
    "model_pipeline.fit(df_train, y_train_t, estimate__sample_weight=w_train_t)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"coefficient\": np.concatenate(\n",
    "            ([model_pipeline[-1].intercept_], model_pipeline[-1].coef_)\n",
    "        )\n",
    "    },\n",
    "    index=[\"intercept\"] + model_pipeline[-1].feature_names_,\n",
    ").T\n",
    "\n",
    "df_test[\"pp_t_glm2\"] = model_pipeline.predict(df_test)\n",
    "df_train[\"pp_t_glm2\"] = model_pipeline.predict(df_train)\n",
    "\n",
    "print(\n",
    "    \"training loss t_glm2:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_glm2\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_glm2:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_glm2\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Total claim amount on test set, observed = {}, predicted = {}\".format(\n",
    "        df[\"ClaimAmountCut\"].values[test].sum(),\n",
    "        np.sum(df[\"Exposure\"].values[test] * df_test[\"pp_t_glm2\"]),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's use a GBM instead as an estimator.\n",
    "# Steps\n",
    "# 1: Define the modelling pipeline. Tip: This can simply be a LGBMRegressor based on X_train_t from before.\n",
    "# 2. Make sure we are choosing the correct objective for our estimator.\n",
    "\n",
    "model_pipeline.fit(X_train_t, y_train_t, estimate__sample_weight=w_train_t)\n",
    "df_test[\"pp_t_lgbm\"] = model_pipeline.predict(X_test_t)\n",
    "df_train[\"pp_t_lgbm\"] = model_pipeline.predict(X_train_t)\n",
    "print(\n",
    "    \"training loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_lgbm\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_lgbm\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's tune the LGBM to reduce overfitting.\n",
    "# Steps:\n",
    "# 1. Define a `GridSearchCV` object with our lgbm pipeline/estimator. Tip: Parameters for a specific step of the pipeline\n",
    "# can be passed by <step_name>__param. \n",
    "\n",
    "# Note: Typically we tune many more parameters and larger grids,\n",
    "# but to save compute time here, we focus on getting the learning rate\n",
    "# and the number of estimators somewhat aligned -> tune learning_rate and n_estimators\n",
    "cv = GridSearchCV(\n",
    "\n",
    ")\n",
    "cv.fit(X_train_t, y_train_t, estimate__sample_weight=w_train_t)\n",
    "\n",
    "df_test[\"pp_t_lgbm\"] = cv.best_estimator_.predict(X_test_t)\n",
    "df_train[\"pp_t_lgbm\"] = cv.best_estimator_.predict(X_train_t)\n",
    "\n",
    "print(\n",
    "    \"training loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_train_t, df_train[\"pp_t_lgbm\"], sample_weight=w_train_t)\n",
    "        / np.sum(w_train_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"testing loss t_lgbm:  {}\".format(\n",
    "        TweedieDist.deviance(y_test_t, df_test[\"pp_t_lgbm\"], sample_weight=w_test_t)\n",
    "        / np.sum(w_test_t)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Total claim amount on test set, observed = {}, predicted = {}\".format(\n",
    "        df[\"ClaimAmountCut\"].values[test].sum(),\n",
    "        np.sum(df[\"Exposure\"].values[test] * df_test[\"pp_t_lgbm\"]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the sorting of the pure premium predictions\n",
    "\n",
    "\n",
    "# Source: https://scikit-learn.org/stable/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html\n",
    "def lorenz_curve(y_true, y_pred, exposure):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    exposure = np.asarray(exposure)\n",
    "\n",
    "    # order samples by increasing predicted risk:\n",
    "    ranking = np.argsort(y_pred)\n",
    "    ranked_exposure = exposure[ranking]\n",
    "    ranked_pure_premium = y_true[ranking]\n",
    "    cumulated_claim_amount = np.cumsum(ranked_pure_premium * ranked_exposure)\n",
    "    cumulated_claim_amount /= cumulated_claim_amount[-1]\n",
    "    cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))\n",
    "    return cumulated_samples, cumulated_claim_amount\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "for label, y_pred in [\n",
    "    (\"LGBM\", df_test[\"pp_t_lgbm\"]),\n",
    "    (\"GLM Benchmark\", df_test[\"pp_t_glm1\"]),\n",
    "    (\"GLM Splines\", df_test[\"pp_t_glm2\"]),\n",
    "]:\n",
    "    ordered_samples, cum_claims = lorenz_curve(\n",
    "        df_test[\"PurePremium\"], y_pred, df_test[\"Exposure\"]\n",
    "    )\n",
    "    gini = 1 - 2 * auc(ordered_samples, cum_claims)\n",
    "    label += f\" (Gini index: {gini: .3f})\"\n",
    "    ax.plot(ordered_samples, cum_claims, linestyle=\"-\", label=label)\n",
    "\n",
    "# Oracle model: y_pred == y_test\n",
    "ordered_samples, cum_claims = lorenz_curve(\n",
    "    df_test[\"PurePremium\"], df_test[\"PurePremium\"], df_test[\"Exposure\"]\n",
    ")\n",
    "gini = 1 - 2 * auc(ordered_samples, cum_claims)\n",
    "label = f\"Oracle (Gini index: {gini: .3f})\"\n",
    "ax.plot(ordered_samples, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)\n",
    "\n",
    "# Random baseline\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\", label=\"Random baseline\")\n",
    "ax.set(\n",
    "    title=\"Lorenz Curves\",\n",
    "    xlabel=\"Fraction of policyholders\\n(ordered by model from safest to riskiest)\",\n",
    "    ylabel=\"Fraction of total claim amount\",\n",
    ")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FODSPS£_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
